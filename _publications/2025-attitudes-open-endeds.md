---
title: "Categorizing Topics Versus Inferring Attitudes: A Theory and Method for Analyzing Open-ended Survey Responses"
collection: publications
permalink: /publication/2025-attitudes-open-endeds
excerpt: "A theory and method for inferring attitudes in open-ended survey responses and other short text documents."
date: 2025-01-24
venue: "Political Analysis"
paperurl: "https://www.cambridge.org/core/journals/political-analysis/article/categorizing-topics-versus-inferring-attitudes-a-theory-and-method-for-analyzing-openended-survey-responses/86230308E694A8DF24EA89FF3A451162"
citation: "Hobbs, William, and Jon Green. 2024. &quot;Categorizing Topics Versus Inferring Attitudes: A Theory and Method for Analyzing Open-ended Survey Responses.&quot; <i>Political Analysis</i>, FirstView."
---

Past work on closed-ended survey responses demonstrates that inferring stable political attitudes requires separating signal from noise in “top of the head” answers to researchers’ questions. We outline a corresponding theory of the open-ended response, in which respondents make narrow, stand-in statements to convey more abstract, general attitudes. We then present a method designed to infer those attitudes. Our approach leverages co-variation with words used relatively frequently across respondents to infer what else they could have said without substantively changing what they meant—linking narrow themes to each other through associations with contextually prevalent words. This reflects the intuition that a respondent may use different specific statements at different points in time to convey similar meaning. We validate this approach using panel data in which respondents answer the same open-ended questions (concerning healthcare policy, most important problems, and evaluations of political parties) at multiple points in time, showing that our method’s output consistently exhibits higher within-subject correlations than hand-coding of narrow response categories, topic modeling, and large language model output. Finally, we show how large language models can be used to complement—but not, at present, substitute—our “implied word” method.